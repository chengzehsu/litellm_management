---
# LiteLLM Proxy Configuration
# This configuration sets up LiteLLM as a data collector that sends
# traces to Langfuse

# General Settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL

# LiteLLM Settings
litellm_settings:
  # Enable Langfuse callback to send traces
  success_callback: ["langfuse"]

  # Langfuse configuration
  langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
  langfuse_secret_key: os.environ/LANGFUSE_SECRET_KEY

  # Optional: Set Langfuse host if using self-hosted instance
  # langfuse_host: os.environ/LANGFUSE_HOST

  # Set default model if needed
  # default_model: "gpt-3.5-turbo"

# Model List Configuration
# Add your models here. Example configurations:
model_list:
  # Example: OpenAI model
  # - model_name: gpt-4
  #   litellm_params:
  #     model: gpt-4
  #     api_key: os.environ/OPENAI_API_KEY

  # Example: Anthropic model
  # - model_name: claude-3-opus
  #   litellm_params:
  #     model: claude-3-opus-20240229
  #     api_key: os.environ/ANTHROPIC_API_KEY

# Router Settings (Optional)
# router_settings:
#   fallbacks: []
#   retries: 3

# Logging Settings
# logging:
#   level: "INFO"
